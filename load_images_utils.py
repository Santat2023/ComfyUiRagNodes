import torch
import clip
from qdrant_client import QdrantClient, models
from . import s3_utils

# –ö–æ–Ω—Ñ–∏–≥ Qdrant
client = QdrantClient(host="localhost", port=6333)

#–ú–æ–¥–µ–ª–∏
device = "cuda" if torch.cuda.is_available() else "cpu"
clip_model, clip_preprocess = clip.load("ViT-B/32", device=device)


# –≠–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ CLIP
def embed_text(text: str):
    with torch.no_grad():
        tokens = clip.tokenize([text]).to(device)
        embedding = clip_model.encode_text(tokens)
        return embedding.cpu().numpy()[0]  # Qdrant –æ–∂–∏–¥–∞–µ—Ç 1D-–≤–µ–∫—Ç–æ—Ä


# –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ –≤ S3 
def upload_to_s3(file_path: str, key: str):
    s3_utils.upload_image(file_path, key)
    url = f"{s3_utils.MINIO_ENDPOINT}/{s3_utils.BUCKET_NAME}/{key}"
    return url


def find_image_by_prompt(query: str, collection_name: str = "images"):
    res = search_images(query, collection_name, top_k=1)

    if not res:
        raise ValueError("‚ö†Ô∏è –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É!")

    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    hit = res[0]
    point_id = hit.id
    payload = hit.payload
    filename = payload.get("filename", "unknown")

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–ª—é—á –≤ S3
    object_name = f"{point_id}_{filename}"
    print(f"üéØ –ù–∞–π–¥–µ–Ω–æ: ID={point_id}, —Ñ–∞–π–ª={filename}, –∫–ª—é—á={object_name}")

    return s3_utils.load_image_bytes_from_s3(object_name)


# –ü–æ–∏—Å–∫ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é
def search_images(query: str, collection_name: str, top_k: int = 5):
    embedding = embed_text(query)

    results = client.search(
        collection_name=collection_name,
        query_vector=embedding,
        limit=top_k
    )
    return results  # —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ScoredPoint
